{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In this notebook a simple Q learner will be trained and evaluated. The Q learner recommends when to buy or sell shares of one particular stock, and in which quantity (in fact it determines the desired fraction of shares in the total portfolio value). One initial attempt was made to train the Q-learner with multiple processes, but it was unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import scipy.optimize as spo\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "from multiprocessing import Pool\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "import recommender.simulator as sim\n",
    "from utils.analysis import value_eval\n",
    "from recommender.agent import Agent\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_THREADS = 1\n",
    "LOOKBACK = -1  # 252*4 + 28\n",
    "STARTING_DAYS_AHEAD = 252\n",
    "POSSIBLE_FRACTIONS = [0.0, 1.0]\n",
    "\n",
    "# Get the data\n",
    "SYMBOL = 'SPY'\n",
    "total_data_train_df = pd.read_pickle('../../data/data_train_val_df.pkl').stack(level='feature')\n",
    "data_train_df = total_data_train_df[SYMBOL].unstack()\n",
    "total_data_test_df = pd.read_pickle('../../data/data_test_df.pkl').stack(level='feature')\n",
    "data_test_df = total_data_test_df[SYMBOL].unstack()\n",
    "if LOOKBACK == -1:\n",
    "    total_data_in_df = total_data_train_df\n",
    "    data_in_df = data_train_df\n",
    "else:\n",
    "    data_in_df = data_train_df.iloc[-LOOKBACK:]\n",
    "    total_data_in_df = total_data_train_df.loc[data_in_df.index[0]:]\n",
    "\n",
    "# Create many agents\n",
    "index = np.arange(NUM_THREADS).tolist()\n",
    "env, num_states, num_actions = sim.initialize_env(total_data_train_df, \n",
    "                                                  SYMBOL, \n",
    "                                                  starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                                  possible_fractions=POSSIBLE_FRACTIONS)\n",
    "agents = [Agent(num_states=num_states, \n",
    "                num_actions=num_actions, \n",
    "                random_actions_rate=0.98, \n",
    "                random_actions_decrease=0.9999,\n",
    "                dyna_iterations=0,\n",
    "                name='Agent_{}'.format(i)) for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_results(results_list, data_in_df, graph=False):\n",
    "    for values in results_list:\n",
    "        total_value = values.sum(axis=1)\n",
    "        print('Sharpe ratio: {}\\nCum. Ret.: {}\\nAVG_DRET: {}\\nSTD_DRET: {}\\nFinal value: {}'.format(*value_eval(pd.DataFrame(total_value))))\n",
    "        print('-'*100)\n",
    "        initial_date = total_value.index[0]\n",
    "        compare_results = data_in_df.loc[initial_date:, 'Close'].copy()\n",
    "        compare_results.name = SYMBOL\n",
    "        compare_results_df = pd.DataFrame(compare_results)\n",
    "        compare_results_df['portfolio'] = total_value\n",
    "        std_comp_df = compare_results_df / compare_results_df.iloc[0]\n",
    "        if graph:\n",
    "            plt.figure()\n",
    "            std_comp_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's show the symbols data, to see how good the recommender has to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe ratio: 0.4566770027925799\n",
      "Cum. Ret.: 3.304502617801047\n",
      "AVG_DRET: 0.0003519913231219332\n",
      "STD_DRET: 0.012235538451970583\n",
      "Final value: 205.54\n"
     ]
    }
   ],
   "source": [
    "print('Sharpe ratio: {}\\nCum. Ret.: {}\\nAVG_DRET: {}\\nSTD_DRET: {}\\nFinal value: {}'.format(*value_eval(pd.DataFrame(data_in_df['Close'].iloc[STARTING_DAYS_AHEAD:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation for agent: Agent_0\n",
      "rsi\n",
      "[[ nan]]\n",
      "feature      Close    High     Low    Open       Volume\n",
      "date                                                   \n",
      "2010-02-26  110.74  111.12  110.11  110.77  173589231.0\n",
      "2010-03-01  111.89  112.00  111.17  111.20  147709644.0\n",
      "2010-03-02  112.20  112.74  112.00  112.37  160992382.0\n",
      "2010-03-03  112.30  112.97  112.02  112.49  150784940.0\n",
      "2010-03-04  112.64  112.80  112.03  112.45  135770353.0\n",
      "2010-03-05  114.25  114.34  113.10  113.37  176118769.0\n",
      "2010-03-08  114.27  114.52  114.07  114.26  114631139.0\n",
      "2010-03-09  114.46  114.99  113.87  113.93  154556674.0\n",
      "2010-03-10  114.97  115.28  114.41  114.51  186088798.0\n",
      "2010-03-11  115.45  115.48  114.35  114.70  160875844.0\n",
      "2010-03-12  115.46  115.97  115.14  115.95  162074761.0\n",
      "2010-03-15  115.49  115.58  114.60  115.26  146816801.0\n",
      "2010-03-16  116.41  116.52  115.49  115.81  168672990.0\n",
      "2010-03-17  117.10  117.48  116.42  116.76  177468085.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-1ea66bdc8347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                        \u001b[0mpossible_fractions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPOSSIBLE_FRACTIONS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                        other_env=env)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/simulator.py\u001b[0m in \u001b[0;36msimulate_period\u001b[0;34m(data_df, symbol, agent, other_env, verbose, learn, starting_days_ahead, possible_fractions)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mrecorded_cash_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_consequences_from_fraction_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/environment.py\u001b[0m in \u001b[0;36mget_consequences_from_fraction_index\u001b[0;34m(self, fraction_index)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_consequences_from_fraction_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mtarget_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_fractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval_to_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_to_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreward_final_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_pos_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pos_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/environment.py\u001b[0m in \u001b[0;36mact_to_target\u001b[0;34m(self, target_fraction)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mshares_increase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwanted_shares\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_shares\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshares_increase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_consequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_consequences_from_fraction_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/environment.py\u001b[0m in \u001b[0;36mget_consequences\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mnew_positions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_positions_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_positions_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_to_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_indicators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mportfolio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/environment.py\u001b[0m in \u001b[0;36mextract_indicators\u001b[0;34m(self, data_df)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_indicators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\" Returns a vector state with the quantized index of all the indicators. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/environment.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_indicators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\" Returns a vector state with the quantized index of all the indicators. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/udacity/Machine Learning Nanodegree/projects/capstone/capstone/recommender/indicator.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, data_df)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mscaled_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/anaconda3/envs/cap_env/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    644\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[1;32m    645\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/anaconda3/envs/cap_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/miguel/anaconda3/envs/cap_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Simulate (with new envs, each time)\n",
    "n_epochs = 15\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    tic = time()\n",
    "    env.reset(STARTING_DAYS_AHEAD)\n",
    "    results_list = sim.simulate_period(total_data_in_df, \n",
    "                                       SYMBOL,\n",
    "                                       agents[0],\n",
    "                                       starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                       possible_fractions=POSSIBLE_FRACTIONS,\n",
    "                                       verbose=False,\n",
    "                                       other_env=env)\n",
    "    toc = time()\n",
    "    print('Epoch: {}'.format(i))\n",
    "    print('Elapsed time: {} seconds.'.format((toc-tic)))\n",
    "    print('Random Actions Rate: {}'.format(agents[0].random_actions_rate))\n",
    "    show_results([results_list], data_in_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.indicators['rsi'].scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_in_df['Close'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_list = sim.simulate_period(total_data_in_df, \n",
    "                                   SYMBOL, agents[0], \n",
    "                                   learn=False, \n",
    "                                   starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                   possible_fractions=POSSIBLE_FRACTIONS,)\n",
    "show_results([results_list], data_in_df, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run the trained agent, with the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First a non-learning test: this scenario would be worse than what is possible (in fact, the q-learner can learn from past samples in the test set without compromising the causality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env, num_states, num_actions = sim.initialize_env(total_data_test_df, \n",
    "                                                  SYMBOL,\n",
    "                                                  starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                                  possible_fractions=POSSIBLE_FRACTIONS)\n",
    "tic = time()\n",
    "results_list = sim.simulate_period(total_data_test_df, \n",
    "                                    SYMBOL,\n",
    "                                    agents[0],\n",
    "                                    learn=False,\n",
    "                                    starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                    possible_fractions=POSSIBLE_FRACTIONS,\n",
    "                                    verbose=False)\n",
    "toc = time()\n",
    "print('Epoch: {}'.format(i))\n",
    "print('Elapsed time: {} seconds.'.format((toc-tic)))\n",
    "print('Random Actions Rate: {}'.format(agents[0].random_actions_rate))\n",
    "show_results([results_list], data_test_df, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now a \"realistic\" test, in which the learner continues to learn from past samples in the test set (it even makes some random moves, though very few)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env, num_states, num_actions = sim.initialize_env(total_data_test_df, \n",
    "                                                  SYMBOL,\n",
    "                                                  starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                                  possible_fractions=POSSIBLE_FRACTIONS)\n",
    "tic = time()\n",
    "results_list = sim.simulate_period(total_data_test_df, \n",
    "                                    SYMBOL,\n",
    "                                    agents[0],\n",
    "                                    learn=True,\n",
    "                                    starting_days_ahead=STARTING_DAYS_AHEAD,\n",
    "                                    possible_fractions=POSSIBLE_FRACTIONS,\n",
    "                                    verbose=False)\n",
    "toc = time()\n",
    "print('Epoch: {}'.format(i))\n",
    "print('Elapsed time: {} seconds.'.format((toc-tic)))\n",
    "print('Random Actions Rate: {}'.format(agents[0].random_actions_rate))\n",
    "show_results([results_list], data_test_df, graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the metrics for \"holding the position\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sharpe ratio: {}\\nCum. Ret.: {}\\nAVG_DRET: {}\\nSTD_DRET: {}\\nFinal value: {}'.format(*value_eval(pd.DataFrame(data_test_df['Close'].iloc[STARTING_DAYS_AHEAD:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../data/simple_q_learner_full_training.pkl', 'wb') as best_agent:\n",
    "    pickle.dump(agents[0], best_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_env",
   "language": "python",
   "name": "cap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
